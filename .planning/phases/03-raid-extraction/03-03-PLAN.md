---
phase: 03-raid-extraction
plan: 03
type: execute
wave: 2
depends_on: ["03-01", "03-02"]
files_modified:
  - src/services/raid_extractor.py
  - tests/services/test_raid_extractor.py
autonomous: true

must_haves:
  truths:
    - "RAIDExtractor extracts all four RAID types from a transcript"
    - "Extraction converts LLM output to domain models with UUIDs and timestamps"
    - "Confidence threshold filters low-confidence extractions"
    - "Each extraction type uses its dedicated prompt"
  artifacts:
    - path: "src/services/raid_extractor.py"
      provides: "RAIDExtractor service orchestrating extraction"
      exports: ["RAIDExtractor", "ExtractionResult"]
  key_links:
    - from: "src/services/raid_extractor.py"
      to: "src/services/llm_client.py"
      via: "constructor injection"
      pattern: "def __init__.*LLMClient"
    - from: "src/services/raid_extractor.py"
      to: "src/extraction/prompts.py"
      via: "import"
      pattern: "from src\\.extraction\\.prompts import"
    - from: "src/services/raid_extractor.py"
      to: "src/extraction/schemas.py"
      via: "response models"
      pattern: "ExtractedActionItem|ExtractedDecision"
    - from: "src/services/raid_extractor.py"
      to: "src/models/"
      via: "domain model conversion"
      pattern: "from src\\.models import"
---

<objective>
Create the RAIDExtractor service that orchestrates extraction of all RAID types from transcripts.

Purpose: Central service that coordinates LLM calls, converts extraction output to domain models, and applies confidence thresholds.
Output: RAIDExtractor service with extract_all() method returning ExtractionResult with domain models
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-raid-extraction/03-RESEARCH.md
@src/services/llm_client.py (from 03-01)
@src/extraction/schemas.py (from 03-01)
@src/extraction/prompts.py (from 03-02)
@src/extraction/date_normalizer.py (from 03-01)
@src/models/action_item.py
@src/models/decision.py
@src/models/risk.py
@src/models/issue.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create RAIDExtractor service</name>
  <files>
    - src/services/raid_extractor.py
  </files>
  <action>
Create src/services/raid_extractor.py:

1. ExtractionResult dataclass:
   - action_items: list[ActionItem]
   - decisions: list[Decision]
   - risks: list[Risk]
   - issues: list[Issue]

2. RAIDExtractor class:
   - __init__(self, llm_client: LLMClient, confidence_threshold: float = 0.5)
   - Store llm_client and confidence_threshold

3. async extract_all(self, transcript_text: str, meeting_id: UUID, meeting_date: datetime) -> ExtractionResult:
   - Call each extraction method in sequence (not parallel - avoid rate limits)
   - Return ExtractionResult with all extracted items

4. Private method _format_transcript(self, utterances: list[Utterance]) -> str:
   - Format utterances as readable transcript for LLM
   - Format: "[{start_time}] {speaker}: {text}" per line
   - This is a static formatting helper

5. Private extraction methods (one per RAID type):

   async _extract_action_items(self, transcript_text: str, meeting_id: UUID, meeting_date: datetime) -> list[ActionItem]:
   - Build prompt using ACTION_ITEM_PROMPT template with transcript_text
   - Call llm_client.extract(prompt, ExtractedActionItems)
   - Filter by confidence >= self.confidence_threshold
   - Convert each ExtractedActionItem to ActionItem domain model:
     * id: uuid4()
     * meeting_id: from parameter
     * description: from extraction
     * assignee_name: from extraction (no identity resolution yet)
     * due_date: normalize_due_date(due_date_raw, meeting_date)
     * status: ActionItemStatus.PENDING
     * source_quote: from extraction
     * confidence: from extraction
   - Return list[ActionItem]

   async _extract_decisions(self, transcript_text: str, meeting_id: UUID) -> list[Decision]:
   - Similar pattern, convert to Decision domain model
   - rationale and alternatives from extraction

   async _extract_risks(self, transcript_text: str, meeting_id: UUID) -> list[Risk]:
   - Similar pattern, convert to Risk domain model
   - Map severity string to RiskSeverity enum

   async _extract_issues(self, transcript_text: str, meeting_id: UUID) -> list[Issue]:
   - Similar pattern, convert to Issue domain model
   - Map priority string to IssuePriority enum
   - status always IssueStatus.OPEN for new extractions

Error handling:
- Wrap each extraction in try/except
- Log errors but continue with other extractions
- Return empty list for failed extraction type rather than failing entire extraction
  </action>
  <verify>
    - `python -c "from src.services.raid_extractor import RAIDExtractor, ExtractionResult; print('OK')"` succeeds
    - RAIDExtractor has extract_all method accepting transcript_text, meeting_id, meeting_date
  </verify>
  <done>RAIDExtractor service extracts all RAID types, converts to domain models, applies confidence threshold</done>
</task>

<task type="auto">
  <name>Task 2: Unit tests with mocked LLM</name>
  <files>
    - tests/services/test_raid_extractor.py
  </files>
  <action>
Create tests/services/test_raid_extractor.py:

1. Fixtures:
   - mock_llm_client: AsyncMock for LLMClient
   - sample_meeting_id: Fixed UUID for tests
   - sample_meeting_date: Fixed datetime for date normalization tests
   - sample_transcript_text: Multi-line formatted transcript

2. Test extract_action_items:
   - Mock llm_client.extract to return ExtractedActionItems with 2 items (confidence 0.8, 0.4)
   - Verify only high-confidence item (0.8) appears in result
   - Verify domain model fields populated (id is UUID, meeting_id matches, status is PENDING)
   - Verify due_date normalized via date_normalizer

3. Test extract_decisions:
   - Mock response with decision including rationale and alternatives
   - Verify conversion to Decision domain model

4. Test extract_risks:
   - Mock response with risk including severity "high"
   - Verify RiskSeverity.HIGH in domain model

5. Test extract_issues:
   - Mock response with issue including priority "critical"
   - Verify IssuePriority.CRITICAL and IssueStatus.OPEN in domain model

6. Test extract_all orchestration:
   - Mock all four extraction calls
   - Verify ExtractionResult contains items from all types
   - Verify calls made in sequence (not parallel)

7. Test extraction error handling:
   - Mock one extraction to raise exception
   - Verify other extractions still complete
   - Verify empty list for failed type

8. Test confidence filtering:
   - Extractor with threshold=0.7
   - Items with confidence 0.6, 0.7, 0.8
   - Verify only 0.7 and 0.8 items returned
  </action>
  <verify>
    - `pytest tests/services/test_raid_extractor.py -v` all tests pass
    - At least 8 tests covering extraction, conversion, error handling, filtering
  </verify>
  <done>RAIDExtractor tested with mocked LLM, confidence filtering verified, error isolation confirmed</done>
</task>

</tasks>

<verification>
- Service imports: `python -c "from src.services.raid_extractor import RAIDExtractor, ExtractionResult"`
- Tests pass: `pytest tests/services/test_raid_extractor.py -v`
- All domain model conversions tested (ActionItem, Decision, Risk, Issue)
</verification>

<success_criteria>
- RAIDExtractor extracts all four RAID types from transcript text
- Extraction output converts to domain models with proper UUIDs and timestamps
- Confidence threshold filters low-confidence extractions
- Extraction errors isolated (one failure doesn't stop others)
- All conversions tested with mocked LLM responses
</success_criteria>

<output>
After completion, create `.planning/phases/03-raid-extraction/03-03-SUMMARY.md`
</output>
