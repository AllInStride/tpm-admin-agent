---
phase: 03-raid-extraction
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - src/extraction/prompts.py
  - tests/extraction/test_prompts.py
autonomous: true

must_haves:
  truths:
    - "System prompts guide LLM to extract RAID items with confidence scores"
    - "Prompts require source quotes for audit trail"
    - "Confidence rubric is explicit and calibrated"
  artifacts:
    - path: "src/extraction/prompts.py"
      provides: "System prompts for each RAID type"
      exports: ["ACTION_ITEM_PROMPT", "DECISION_PROMPT", "RISK_PROMPT", "ISSUE_PROMPT"]
  key_links:
    - from: "src/extraction/prompts.py"
      to: "RAIDExtractor (Plan 03)"
      via: "imported constants"
      pattern: "from src\\.extraction\\.prompts import"
---

<objective>
Create extraction prompts for each RAID type with explicit confidence rubrics and source quote requirements.

Purpose: Well-crafted prompts are critical for extraction quality. Separate prompts per type yield better results than one mega-prompt.
Output: Four system prompts (action items, decisions, risks, issues) with confidence calibration guidelines
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-raid-extraction/03-RESEARCH.md
@src/models/action_item.py
@src/models/decision.py
@src/models/risk.py
@src/models/issue.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create extraction prompts</name>
  <files>
    - src/extraction/prompts.py
  </files>
  <action>
Create src/extraction/prompts.py with four system prompts as module-level constants:

1. ACTION_ITEM_PROMPT:
   - Extract action items (commitments, tasks assigned, things people agreed to do)
   - Required: description, source_quote (exact quote), confidence
   - Optional: assignee_name (as mentioned in transcript), due_date_raw (natural language)
   - Confidence rubric:
     * 0.9-1.0: Explicit commitment with clear owner ("I will do X", "John, can you handle Y")
     * 0.7-0.9: Implied commitment or clear task with likely owner ("We need to... John?")
     * 0.5-0.7: Task mentioned but owner unclear or tentative
     * Below 0.5: Do not extract (too uncertain)
   - Instruction: Return ONLY items with confidence >= 0.5
   - Instruction: Distinguish "discussion about tasks" from "actual commitments"

2. DECISION_PROMPT:
   - Extract decisions (choices made, agreements reached, directions set)
   - Required: description, source_quote, confidence
   - Optional: rationale (why decided), alternatives (other options discussed)
   - Confidence rubric:
     * 0.9-1.0: Explicit decision statement ("We decided to", "Let's go with")
     * 0.7-0.9: Strong consensus or clear direction ("Everyone agrees", "That makes sense, let's do it")
     * 0.5-0.7: Implied decision or soft agreement
     * Below 0.5: Do not extract
   - Instruction: Distinguish "decision made" from "decision being discussed"

3. RISK_PROMPT:
   - Extract risks (potential problems, concerns about future, what could go wrong)
   - Required: description, severity, source_quote, confidence
   - Optional: impact (what happens if risk materializes), mitigation (how to address), owner_name
   - Severity guidelines:
     * critical: Project/business failure, major deadline miss
     * high: Significant impact, requires immediate attention
     * medium: Moderate impact, should be tracked
     * low: Minor concern, nice to track
   - Confidence rubric:
     * 0.9-1.0: Explicit risk statement ("The risk is", "We might fail if")
     * 0.7-0.9: Clear concern expressed ("I'm worried about", "That could be a problem")
     * 0.5-0.7: Implied concern or cautionary note
   - Instruction: Distinguish risks (future potential) from issues (current problems)

4. ISSUE_PROMPT:
   - Extract issues (current problems, blockers, things not working)
   - Required: description, priority, source_quote, confidence
   - Optional: impact, owner_name
   - Status is always "open" for new extractions
   - Priority guidelines (same as severity above)
   - Confidence rubric similar to risks
   - Instruction: Distinguish issues (current problems) from risks (potential problems)

All prompts should:
- Put the extraction instructions AFTER the transcript placeholder (per research on "lost in the middle")
- Include format: "Transcript:\n{transcript}\n\n[extraction instructions]"
- Remind LLM to extract ONLY from the provided transcript, not from general knowledge
</action>
  <verify>
    - `python -c "from src.extraction.prompts import ACTION_ITEM_PROMPT, DECISION_PROMPT, RISK_PROMPT, ISSUE_PROMPT; print('All prompts loaded')"` succeeds
    - Each prompt contains confidence rubric (grep for "0.9-1.0" pattern)
    - Each prompt requires source_quote
  </verify>
  <done>Four RAID prompts with explicit confidence rubrics, source quote requirements, and type-specific guidance</done>
</task>

<task type="auto">
  <name>Task 2: Test prompts exist and contain required elements</name>
  <files>
    - tests/extraction/test_prompts.py
  </files>
  <action>
Create tests/extraction/test_prompts.py:

1. Test each prompt is a non-empty string
2. Test each prompt contains confidence rubric indicators (0.9, 0.7, 0.5)
3. Test each prompt mentions source_quote requirement
4. Test ACTION_ITEM_PROMPT mentions assignee and due_date
5. Test DECISION_PROMPT mentions rationale and alternatives
6. Test RISK_PROMPT mentions severity levels (critical, high, medium, low)
7. Test ISSUE_PROMPT mentions priority levels
8. Test prompts contain transcript placeholder guidance

These are structural tests ensuring prompts meet requirements - actual extraction quality is tested via integration tests.
  </action>
  <verify>
    - `pytest tests/extraction/test_prompts.py -v` all tests pass
    - At least 8 tests covering prompt structure
  </verify>
  <done>Prompt structure tests verify all required elements present</done>
</task>

</tasks>

<verification>
- Prompts import: `python -c "from src.extraction.prompts import *"`
- Each prompt has confidence rubric: `grep -c "0.9" src/extraction/prompts.py` returns 4
- Tests pass: `pytest tests/extraction/test_prompts.py -v`
</verification>

<success_criteria>
- Four distinct prompts for action items, decisions, risks, issues
- Each prompt has explicit confidence calibration rubric
- Each prompt requires source_quote extraction
- Each prompt has type-specific extraction guidance
- Prompt structure tests validate required elements
</success_criteria>

<output>
After completion, create `.planning/phases/03-raid-extraction/03-02-SUMMARY.md`
</output>
