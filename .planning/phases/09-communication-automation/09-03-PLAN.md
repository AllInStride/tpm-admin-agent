---
phase: 09-communication-automation
plan: 03
type: execute
wave: 2
depends_on: ["09-01"]
files_modified:
  - src/communication/generators/escalation.py
  - src/communication/generators/talking_points.py
  - tests/communication/test_escalation_generator.py
  - tests/communication/test_talking_points_generator.py
autonomous: true

must_haves:
  truths:
    - "Escalation emails follow Problem-Impact-Ask format"
    - "Escalations always include 2-3 options with pros/cons"
    - "Escalations have explicit deadline for decision"
    - "Talking points include anticipated Q&A section"
    - "Talking points cover risk/concern and resource question categories"
  artifacts:
    - path: "src/communication/generators/escalation.py"
      provides: "EscalationGenerator for COM-03"
      exports: ["EscalationGenerator"]
    - path: "src/communication/generators/talking_points.py"
      provides: "TalkingPointsGenerator for COM-04"
      exports: ["TalkingPointsGenerator"]
  key_links:
    - from: "src/communication/generators/escalation.py"
      to: "src/services/llm_client.py"
      via: "LLMClient.extract for structured output"
      pattern: "llm.*extract.*EscalationOutput"
    - from: "src/communication/generators/talking_points.py"
      to: "src/services/llm_client.py"
      via: "LLMClient.extract for structured output"
      pattern: "llm.*extract.*TalkingPointsOutput"
---

<objective>
Implement escalation email generator (COM-03) and exec talking points generator (COM-04). Escalations use EscalationRequest input (not StatusData). Talking points use StatusData plus meeting_type context.

Purpose: Fulfills COM-03 (escalation emails with Problem-Impact-Ask format) and COM-04 (exec talking points with Q&A).
Output: Working generators that produce focused communication artifacts.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/09-communication-automation/09-CONTEXT.md
@.planning/phases/09-communication-automation/09-RESEARCH.md
@.planning/phases/09-communication-automation/09-01-SUMMARY.md

# Infrastructure from Plan 01
@src/communication/schemas.py
@src/communication/prompts.py
@src/communication/generators/base.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: EscalationGenerator (COM-03)</name>
  <files>
    src/communication/generators/escalation.py
    tests/communication/test_escalation_generator.py
  </files>
  <action>
**escalation.py:**
- `EscalationGenerator(BaseGenerator)`:
  - `async def generate(self, request: EscalationRequest) -> GeneratedArtifact`:
    1. Format options for prompt using `_format_options(request.options) -> str`:
       - For each option: "Option {A,B,C}: {description}\n  Pros: {pros}\n  Cons: {cons}"
    2. Build prompt using ESCALATION_PROMPT.format() with:
       - problem_description, timeline_impact (or "Not specified"), resource_impact, business_impact
       - history_context (or "No prior history")
       - options_data from formatted options
       - decision_deadline formatted as YYYY-MM-DD
    3. Call `self._llm.extract(prompt, EscalationOutput)` for structured output
    4. **Validate output:**
       - `if not output.options or len(output.options) < 2: raise ValueError("Escalation must have at least 2 options")`
       - `if not output.deadline: raise ValueError("Escalation must have explicit deadline")`
    5. Build context from output.model_dump() + recipient, generated_at
    6. Render template: `_, plain_text = self._render_template('escalation_email', context)`
       - Escalations are email, so markdown not needed (plain_text for both)
    7. Return GeneratedArtifact with type='escalation', metadata={'subject': output.subject, 'deadline': output.deadline, 'option_count': len(output.options)}

Per CONTEXT.md requirements:
- Problem-Impact-Ask format - structured in prompt
- Explicit deadline ("Decision needed by [date]") - validated
- Always include options (A, B, or C) - validated min 2 options
- Matter-of-fact tone - instruction in prompt
- Recipients: user specifies via request.recipient

Create tests verifying:
- Options are formatted correctly
- Validation rejects < 2 options
- Validation rejects missing deadline
- Output contains subject line
  </action>
  <verify>
`uv run pytest tests/communication/test_escalation_generator.py -v` passes
  </verify>
  <done>
EscalationGenerator produces Problem-Impact-Ask emails with validated options and explicit deadlines
  </done>
</task>

<task type="auto">
  <name>Task 2: TalkingPointsGenerator (COM-04)</name>
  <files>
    src/communication/generators/talking_points.py
    tests/communication/test_talking_points_generator.py
  </files>
  <action>
**talking_points.py:**
- `TalkingPointsGenerator(BaseGenerator)`:
  - `async def generate(self, data: StatusData, *, meeting_type: str = "exec_review") -> GeneratedArtifact`:
    1. Build prompt using TALKING_POINTS_PROMPT.format() with:
       - project_name, meeting_type
       - period_start, period_end
       - key_progress (top 5 completed items)
       - decisions (top 3)
       - risks, issues (top 5 each)
       - blockers (all)
       - metrics: `_format_metrics(data)` returning item counts, velocity, overdue count
    2. Call `self._llm.extract(prompt, TalkingPointsOutput)` for structured output
    3. **Validate Q&A coverage:**
       - Extract categories: `qa_categories = {q['category'] for q in output.anticipated_qa}`
       - Required: {'risk', 'resource'}
       - If missing categories, log warning (don't fail - LLM may have valid reason)
    4. Build context from output.model_dump() + project_name, meeting_type, generated_at
    5. Render templates
    6. Return GeneratedArtifact with type='talking_points', metadata={'point_count': len(output.key_points), 'qa_count': len(output.anticipated_qa)}

  - Helper: `_format_metrics(data: StatusData) -> str`:
    ```
    Items completed: {len(completed_items)}
    Items opened: {len(new_items)}
    Net velocity: {item_velocity:+d}
    Currently open: {len(open_items)}
    Overdue items: {overdue_count}
    Active risks: {len(risks)}
    Open issues: {len(issues)}
    Blockers: {len(blockers)}
    Meetings held: {len(meetings_held)}
    ```

Per CONTEXT.md requirements:
- Key bullet points + anticipated Q&A section - in schema
- Narrative focus with supporting data - in prompt
- Comprehensive Q&A: risk/concern + resource + other - validated with warning

Create tests verifying:
- Key points are generated
- Q&A section exists with categories
- Metrics are formatted correctly
  </action>
  <verify>
`uv run pytest tests/communication/test_talking_points_generator.py -v` passes
  </verify>
  <done>
TalkingPointsGenerator produces narrative talking points with anticipated Q&A covering risk/resource categories
  </done>
</task>

</tasks>

<verification>
- [ ] `src/communication/generators/escalation.py` and `talking_points.py` exist
- [ ] `uv run pytest tests/communication/test_escalation_generator.py tests/communication/test_talking_points_generator.py -v` passes
- [ ] EscalationGenerator validates options (min 2) and deadline
- [ ] TalkingPointsGenerator includes Q&A with category coverage
</verification>

<success_criteria>
- EscalationGenerator fulfills COM-03: Problem-Impact-Ask format, options with pros/cons, explicit deadline
- TalkingPointsGenerator fulfills COM-04: narrative with key points, anticipated Q&A (risk, resource, other)
- Both generators integrate with LLMClient structured outputs
- Validation catches malformed outputs
</success_criteria>

<output>
After completion, create `.planning/phases/09-communication-automation/09-03-SUMMARY.md`
</output>
